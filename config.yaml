CFG:
  # gpt2, gpt2-medium, gpt2-large, gpt2-xl
  # t5-small, t5-base, t5-large, t5-3b
  # t5-v1_1-small, t5-v1_1-base, t5-v1_1-large, t5-v1_1-xl
  # t5-small-lm-adapt, t5-base-lm-adapt, t5-large-lm-adapt, t5-xl-lm-adapt
  model_type: gpt2 # gpt2, t5

  # configuration reconstructed according to the paper
  data_path: snoop2head/common_crawl # one of snoop2head/common_crawl, c4, openwebtext
  min_prefix_length: 5 # prefix ranges 5 ~ 10 according to the paper
  max_prefix_length: 10 # prefix ranges 5 ~ 10 according to the paper
  generate_token_length: 256 # +256 token to the given prefix
  top_n: 40 # n = 40 (or k = 40)
  num_queries: 200000 # 200K sampling(or generation) per each scheme
  num_return_sequences: 1 
  num_reranking_top_samples: 100 # reranking top 100 items x 3 generation methods x 6 metrics
  window_size: 50

  # configuration for the model's inference / reranking
  inference_batch_size: 40
  num_reranking: 150 # number of reranking
  rerank_batch_size: 1
  device_ids: # if -1 use cpu(not recommended), elif 0 use single gpu, else use multiple gpus
    - 0
  fp16: false
  num_beams: 5
  repetition_penalty: 1.3
  no_repeat_ngram_size: 3

  # other configs
  seed: 42
  inference_result_file_name: ./results/generation_result.csv
  rerank_result_file_name: ./results/memorization_assessed.csv
  top_100_path: ./results
